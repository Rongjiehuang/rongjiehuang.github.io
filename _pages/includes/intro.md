
My research interest includes **Multi-modal Large Language Model, Omni-modal Generative Models, and Audio-Visual Language Processing**. I am at CUHK, supervised by Prof. Helen Meng and Prof. Xixin Wu. I graduated from College of Computer Science, [Zhejiang University](https://www.zju.edu.cn/english/), supervised by Prof. Zhou Zhao. I also obtained Bachelor‚Äôs degree at Zhejiang University. I have published **first-author papers** at the top international AI conferences such as **NeurIPS/ICLR/ICML/ACL/IJCAI**. I developed a few well-known Speech/NLP algorithms including:

<!-- **Rongjie Huang (ÈªÑËûçÊù∞)**  -->
<!-- is at [Seamless Team](https://ai.meta.com/research/seamless-communication/) at **FAIR**.  -->

- **Multimodal LLMs**: [Seamless-Interaction (LLama4+Dyadic Motion Diffusion)](https://ai.meta.com/blog/seamless-interaction-natural-conversational-dynamics/), [AudioGPT](https://github.com/AIGC-Audio/AudioGPT), [UniAudio](https://arxiv.org/abs/2310.00704), [Make-A-Voice](https://arxiv.org/abs/2305.19269)
- **Omini-modal Audio Generative Models**: [Lumina-T2X (omini-modal)](https://github.com/Alpha-VLLM/Lumina-T2X), [Make-An-Audio](https://github.com/Text-to-Audio/Make-An-Audio), [Make-An-Audio-2](https://github.com/bytedance/Make-An-Audio-2), [FastDiff](https://github.com/Rongjiehuang/FastDiff), [GenerSpeech](https://openreview.net/pdf?id=dmCyoqxEwHf)
- **Multimodal Translation**: [TranSpeech](https://arxiv.org/abs/2205.12523), and [AV-TranSpeech](https://arxiv.org/abs/2305.15403)


<!-- During my graduate study, I was lucky to collaborate with the CMU Speech Team led by [Prof. Shinji Watanabe](https://scholar.google.com/citations?user=U5xRA6QAAAAJ), and Audio Research Team at Zhejiang University.  -->

<!-- I was grateful to intern or collaborate at TikTok, Shanghai AI Lab, Tencent Seattle Lab, Alibaba Qwen, with [Yi Ren](https://github.com/RayeRen), [Jinglin Liu](https://github.com/MoonInTheRiver), [Chunlei Zhang](https://scholar.google.com/citations?user=NCKZGb0AAAAJ) and [Dong Yu](https://scholar.google.com/citations?user=tMY31_gAAAAJ). -->



<!-- In 2024, I lead or participate in the following research topics:
- Speech/NLP: multimodal generation and translation
- Large Language Models (LLMs): Audio/Visual
- Diffusion models: Image/Audio/3D -->


# üî• News

<style>
  .scrollable {
    max-height: 260px; /* ËÆæÁΩÆÊúÄÂ§ßÈ´òÂ∫¶ */
    overflow-y: scroll; /* ËÆæÁΩÆÂûÇÁõ¥ÊªöÂä®Êù° */
  }
</style>

<div class="scrollable">
  <ul>
    <li><strong>2025.04</strong>: <font color="red"> I am awarded the Best Thesis Award by the Electrical Engineering Association! </font></li>
    <li><strong>2025.02</strong>: 4 papers are accepted by ICLR 2025! </li>
    <li><strong>2025.01</strong>: 1 paper is accepted by AAAI 2025!  </li>
    <li><strong>2024.10</strong>: 6 papers are accepted by NeurIPS 2024! </li>
    <li><strong>2024.05</strong>: 6 papers are accepted by ACL 2024! (main conference and findings)! </li>
    <li><strong>2024.05</strong>: 3 papers are accepted by ICML 2024!</li>
    <li><strong>2024.03</strong>: 1 paper is accepted by NAACL 2024 main conference!</li>
    <li><strong>2024.01</strong>: 1 paper is accepted by ICLR 2024!</li>
    <li><strong>2023.11</strong>: 2 papers are accepted by AAAI 2024 main / AAAI 2024 demo!</li>
    <li><strong>2023.10</strong>: <font color="red"> I am awarded ByteDance Scholar Fellowship, and Chu Kochen Presidential Scholarship! </font></li>
    <li><strong>2023.10</strong>: <a href="https://twitter.com/_akhaliq/status/1710112638422642732">UniAudio</a> released!</li>
    <li><strong>2023.09</strong>: One paper is accepted by EMNLP 2023!</li>
    <li><strong>2023.07</strong>: One paper is accepted by ACM-MM 2023! </li>
    <li><strong>2023.06</strong>: One paper is accepted by ICCV 2023! </li>
    <li><strong>2023.05</strong>: 8 papers are accepted by ACL 2023 (main conference and findings)! Thanks to my co-authors! </li>
    <li><strong>2023.04</strong>:  <a href="https://github.com/AIGC-Audio/AudioGPT">AudioGPT</a> and <a href="https://github.com/yangdongchao/AcademiCodec">HiFi-Codec</a> released!  </li>
    <li><strong>2023.04</strong>: One papers is accepted by ICML 2023! </li>
    <li><strong>2023.02</strong>: Make-An-Audio released! Media coverage: <a href="https://mp.weixin.qq.com/s/fphIJ13RWRIgGNTwYO06bw">Heart of Machine</a>, <a href="https://zhuanlan.zhihu.com/p/605228032">ByteDance</a> and <a href="https://twitter.com/_akhaliq/status/1619589070329348096">Twitter</a> </li>
    <li><strong>2023.01</strong>: One papers is accepted by ICLR 2023! </li>
    <li><strong>2022.09</strong>: Two papers are accepted by NeurIPS 2022! </li>
  </ul>
</div>

