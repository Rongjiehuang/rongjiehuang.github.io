{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "iRHBUsgAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Rongjie Huang", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=iRHBUsgAAAAJ&citpid=3", "affiliation": "FAIR, Zhejiang University", "organization": 1118375729466322660, "interests": ["Multimedia Computing", "Speech", "Natural Language Processing"], "email_domain": "@zju.edu.cn", "homepage": "http://rongjiehuang.github.io/", "citedby": 3843, "publications": {"iRHBUsgAAAAJ:KlAtU1dfN6UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Make-An-Audio: Text-to-audio generation with prompt-enhanced diffusion models", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:KlAtU1dfN6UC", "num_citations": 454, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14204166098403262471", "cites_id": ["14204166098403262471"]}, "iRHBUsgAAAAJ:_kc_bZDykSQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Audiogpt: Understanding and generating speech, music, sound, and talking head", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:_kc_bZDykSQC", "num_citations": 316, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10625702268349818855", "cites_id": ["10625702268349818855"]}, "iRHBUsgAAAAJ:Se3iqnhoufwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech", "pub_year": "2022"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:Se3iqnhoufwC", "num_citations": 235, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9223505264010200019", "cites_id": ["9223505264010200019"]}, "iRHBUsgAAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis", "pub_year": "2022"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:YsMSGLbcyi4C", "num_citations": 212, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2088823687859195371", "cites_id": ["2088823687859195371"]}, "iRHBUsgAAAAJ:M3ejUd6NZC8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Hifi-codec: Group-residual vector quantization for high fidelity audio codec", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:M3ejUd6NZC8C", "num_citations": 181, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16848014406171770614", "cites_id": ["16848014406171770614"]}, "iRHBUsgAAAAJ:HDshCWvjkbEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Uniaudio: An audio foundation model toward universal audio generation", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:HDshCWvjkbEC", "num_citations": 180, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6416249801268158267", "cites_id": ["6416249801268158267"]}, "iRHBUsgAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Bilateral denoising diffusion models", "pub_year": "2021"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:2osOgNQ5qMEC", "num_citations": 167, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9819196866307115344,8410367099889699879", "cites_id": ["9819196866307115344", "8410367099889699879"]}, "iRHBUsgAAAAJ:eQOLeE2rZwMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "GenerSpeech: Towards Style Transfer for Generalizable Out-Of-Domain Text-to-Speech", "pub_year": "2022"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:eQOLeE2rZwMC", "num_citations": 128, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13191102001080802763", "cites_id": ["13191102001080802763"]}, "iRHBUsgAAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Multi-singer: Fast multi-singer singing voice vocoder with a large-scale corpus", "pub_year": "2021"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:zYLM7Y9cAGgC", "num_citations": 125, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2358855456085494126", "cites_id": ["2358855456085494126"]}, "iRHBUsgAAAAJ:Zph67rFs4hoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Instructtts: Modelling expressive tts in discrete latent space with natural language style prompt", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:Zph67rFs4hoC", "num_citations": 121, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16890941306445601074", "cites_id": ["16890941306445601074"]}, "iRHBUsgAAAAJ:e5wmG9Sq2KIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Chat-scene: Bridging 3d scene and large language models with object identifiers", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:e5wmG9Sq2KIC", "num_citations": 114, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17772435643977512045,9819100763177515104", "cites_id": ["17772435643977512045", "9819100763177515104"]}, "iRHBUsgAAAAJ:LkGwnXOMwfcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "M4Singer: a Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus", "pub_year": "2022"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:LkGwnXOMwfcC", "num_citations": 111, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1634477637622720706", "cites_id": ["1634477637622720706"]}, "iRHBUsgAAAAJ:7PzlFSSx8tAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Make-an-audio 2: Temporal-enhanced text-to-audio generation", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:7PzlFSSx8tAC", "num_citations": 101, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=13517159671265524532", "cites_id": ["13517159671265524532"]}, "iRHBUsgAAAAJ:2P1L_qKh6hAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:2P1L_qKh6hAC", "num_citations": 98, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11096367345508634509", "cites_id": ["11096367345508634509"]}, "iRHBUsgAAAAJ:1sJd4Hv_s6UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mega-tts: Zero-shot text-to-speech at scale with intrinsic inductive bias", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:1sJd4Hv_s6UC", "num_citations": 95, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15692405188854768212", "cites_id": ["15692405188854768212"]}, "iRHBUsgAAAAJ:4JMBOYKVnBMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Real3d-portrait: One-shot realistic 3d talking portrait synthesis", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:4JMBOYKVnBMC", "num_citations": 79, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4610920972123504276", "cites_id": ["4610920972123504276"]}, "iRHBUsgAAAAJ:roLk4NBRz8UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SingGAN: Generative Adversarial Network For High-Fidelity Singing Voice Generation", "pub_year": "2022"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:roLk4NBRz8UC", "num_citations": 73, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1698516610881405813", "cites_id": ["1698516610881405813"]}, "iRHBUsgAAAAJ:4TOpqqG69KYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Geneface++: Generalized and stable real-time audio-driven 3d talking face generation", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:4TOpqqG69KYC", "num_citations": 60, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=408392048948677980", "cites_id": ["408392048948677980"]}, "iRHBUsgAAAAJ:ULOm3_A8WrAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation", "pub_year": "2022"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:ULOm3_A8WrAC", "num_citations": 57, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15292967138191810315", "cites_id": ["15292967138191810315"]}, "iRHBUsgAAAAJ:GnPB-g6toBAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Uniaudio: Towards universal audio generation with large language models", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:GnPB-g6toBAC", "num_citations": 50, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7553768456221728553", "cites_id": ["7553768456221728553"]}, "iRHBUsgAAAAJ:9yKSN-GCB0IC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model", "pub_year": "2021"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:9yKSN-GCB0IC", "num_citations": 45, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15315350381922618364", "cites_id": ["15315350381922618364"]}, "iRHBUsgAAAAJ:P5F9QuxV20EC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Simplespeech 2: Towards simple and efficient text-to-speech with flow-based scalar latent transformer diffusion models", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:P5F9QuxV20EC", "num_citations": 44, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14605884257628173023", "cites_id": ["14605884257628173023"]}, "iRHBUsgAAAAJ:vV6vV6tmYwMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Make-a-voice: Revisiting voice large language models as scalable multilingual and multitask learners", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:vV6vV6tmYwMC", "num_citations": 44, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12763739973148023166,1604035471441920381", "cites_id": ["12763739973148023166", "1604035471441920381"]}, "iRHBUsgAAAAJ:_Qo2XoVZTnwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Stylesinger: Style transfer for out-of-domain singing voice synthesis", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:_Qo2XoVZTnwC", "num_citations": 39, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9942954876960767861", "cites_id": ["9942954876960767861"]}, "iRHBUsgAAAAJ:maZDTaKrznsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:maZDTaKrznsC", "num_citations": 35, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14417271216765405515", "cites_id": ["14417271216765405515"]}, "iRHBUsgAAAAJ:hMod-77fHWUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Frieren: Efficient video-to-audio generation with rectified flow matching", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:hMod-77fHWUC", "num_citations": 34, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12114926302115863266,4245251456510093791", "cites_id": ["12114926302115863266", "4245251456510093791"]}, "iRHBUsgAAAAJ:YOwf2qJgpHMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Mixspeech: Cross-modality self-learning with audio-visual stream mixup for visual speech translation and recognition", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:YOwf2qJgpHMC", "num_citations": 30, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2782308253975293934", "cites_id": ["2782308253975293934"]}, "iRHBUsgAAAAJ:ns9cj8rnVeAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Uniaudio 1.5: Large language model-driven audio codec is a few-shot audio task learner", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:ns9cj8rnVeAC", "num_citations": 29, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9610513952615411361", "cites_id": ["9610513952615411361"]}, "iRHBUsgAAAAJ:aqlVkmm33-oC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Rmssinger: Realistic-music-score based singing voice synthesis", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:aqlVkmm33-oC", "num_citations": 29, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10533965237930127174", "cites_id": ["10533965237930127174"]}, "iRHBUsgAAAAJ:QIV2ME_5wuYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Fluentspeech: Stutter-oriented automatic speech editing with context-aware diffusion models", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:QIV2ME_5wuYC", "num_citations": 26, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7922113499487900716", "cites_id": ["7922113499487900716"]}, "iRHBUsgAAAAJ:mVmsd5A6BfQC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Vit-tts: visual text-to-speech with scalable diffusion transformer", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:mVmsd5A6BfQC", "num_citations": 26, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14147897746414758233", "cites_id": ["14147897746414758233"]}, "iRHBUsgAAAAJ:4DMP91E08xMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Clapspeech: Learning prosody from text context with contrastive language-audio pre-training", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:4DMP91E08xMC", "num_citations": 24, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11061604911451895335", "cites_id": ["11061604911451895335"]}, "iRHBUsgAAAAJ:j3f4tGmQtD8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Language-codec: Reducing the gaps between discrete codec representation and speech language models", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:j3f4tGmQtD8C", "num_citations": 23, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15333061713300156908", "cites_id": ["15333061713300156908"]}, "iRHBUsgAAAAJ:NaGl4SEjCO4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Omnibind: Large-scale omni multimodal representation via binding spaces", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:NaGl4SEjCO4C", "num_citations": 22, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15812878167712032072", "cites_id": ["15812878167712032072"]}, "iRHBUsgAAAAJ:bEWYMUwI8FkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "3D-Speaker-Toolkit: An Open-Source Toolkit for Multimodal Speaker Verification and Diarization", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:bEWYMUwI8FkC", "num_citations": 21, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6337081796054164039", "cites_id": ["6337081796054164039"]}, "iRHBUsgAAAAJ:blknAaTinKkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Controlspeech: Towards simultaneous zero-shot speaker cloning and zero-shot language style control with decoupled codec", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:blknAaTinKkC", "num_citations": 21, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=403976369840040732", "cites_id": ["403976369840040732"]}, "iRHBUsgAAAAJ:9ZlFYXVOiuMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Av-transpeech: Audio-visual robust speech-to-speech translation", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:9ZlFYXVOiuMC", "num_citations": 21, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3031965472100444937", "cites_id": ["3031965472100444937"]}, "iRHBUsgAAAAJ:yD5IFk8b50cC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Extending multi-modal contrastive representations", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:yD5IFk8b50cC", "num_citations": 20, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17544396253213667366", "cites_id": ["17544396253213667366"]}, "iRHBUsgAAAAJ:NMxIlDl6LWMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Audiolcm: Text-to-audio generation with latent consistency models", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:NMxIlDl6LWMC", "num_citations": 18, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14141182423302675181", "cites_id": ["14141182423302675181"]}, "iRHBUsgAAAAJ:hqOjcs7Dif8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "VarietySound: Timbre-Controllable Video to Sound Generation via Unsupervised Information Disentanglement", "pub_year": "2022"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:hqOjcs7Dif8C", "num_citations": 18, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2333085745106701374", "cites_id": ["2333085745106701374"]}, "iRHBUsgAAAAJ:IWHjjKOFINEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Fastdiff 2: Revisiting and incorporating gans and diffusion models in high-fidelity speech synthesis", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:IWHjjKOFINEC", "num_citations": 17, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5865263432404926087", "cites_id": ["5865263432404926087"]}, "iRHBUsgAAAAJ:ldfaerwXgEUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Tcsinger: Zero-shot singing voice synthesis with style transfer and multi-level style control", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:ldfaerwXgEUC", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17539281131458133416", "cites_id": ["17539281131458133416"]}, "iRHBUsgAAAAJ:BqipwSGYUEgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Freebind: Free lunch in unified multimodal space via knowledge fusion", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:BqipwSGYUEgC", "num_citations": 16, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16449970934698418261", "cites_id": ["16449970934698418261"]}, "iRHBUsgAAAAJ:M05iB0D1s5AC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:M05iB0D1s5AC", "num_citations": 15, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9226530088467420528", "cites_id": ["9226530088467420528"]}, "iRHBUsgAAAAJ:bFI3QPDXJZMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "VoxDialogue: Can Spoken Dialogue Systems Understand Information Beyond Words?", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:bFI3QPDXJZMC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6165359200339987314", "cites_id": ["6165359200339987314"]}, "iRHBUsgAAAAJ:g5m5HwL7SMYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Lumina-next: Making lumina-t2x stronger and faster with next-dit", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:g5m5HwL7SMYC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15035790787388684947", "cites_id": ["15035790787388684947"]}, "iRHBUsgAAAAJ:JV2RwH3_ST0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Robust singing voice transcription serves synthesis", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:JV2RwH3_ST0C", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=425946306691459202", "cites_id": ["425946306691459202"]}, "iRHBUsgAAAAJ:RHpTSmoSYBkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Transface: Unit-based audio-visual speech synthesizer for talking head translation", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:RHpTSmoSYBkC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17335056964216082744", "cites_id": ["17335056964216082744"]}, "iRHBUsgAAAAJ:L8Ckcad2t8MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Detector guidance for multi-object text-to-image generation", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:L8Ckcad2t8MC", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11690234322079605255", "cites_id": ["11690234322079605255"]}, "iRHBUsgAAAAJ:r0BpntZqJG4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Prompt-singer: Controllable singing-voice-synthesis with natural language prompt", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:r0BpntZqJG4C", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10305936552962695765", "cites_id": ["10305936552962695765"]}, "iRHBUsgAAAAJ:qUcmZB5y_30C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Prosody-tts: Improving prosody with masked autoencoder and conditional diffusion model for expressive text-to-speech", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:qUcmZB5y_30C", "num_citations": 13, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6700848495971978849", "cites_id": ["6700848495971978849"]}, "iRHBUsgAAAAJ:pyW8ca7W8N0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Techsinger: Technique controllable multilingual singing voice synthesis via flow matching", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:pyW8ca7W8N0C", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15775397014533613515", "cites_id": ["15775397014533613515"]}, "iRHBUsgAAAAJ:lSLTfruPkqcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Audiolcm: Efficient and high-quality text-to-audio generation with minimal inference steps", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:lSLTfruPkqcC", "num_citations": 10, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5288166097147531298", "cites_id": ["5288166097147531298"]}, "iRHBUsgAAAAJ:TQgYirikUcIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Unisinger: Unified end-to-end singing voice synthesis with cross-modality information matching", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:TQgYirikUcIC", "num_citations": 10, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16431312941436321345", "cites_id": ["16431312941436321345"]}, "iRHBUsgAAAAJ:hFOr9nPyWt4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Speech-to-speech translation with discrete-unit-based style transfer", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:hFOr9nPyWt4C", "num_citations": 10, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12603739386764076775", "cites_id": ["12603739386764076775"]}, "iRHBUsgAAAAJ:O3NaXMp0MMsC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Instructspeech: Following speech editing instructions via large language models", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:O3NaXMp0MMsC", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16502846156364932331", "cites_id": ["16502846156364932331"]}, "iRHBUsgAAAAJ:hC7cP41nSMkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Contrastive token-wise meta-learning for unseen performer visual temporal-aligned translation", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:hC7cP41nSMkC", "num_citations": 9, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6919126084284083155", "cites_id": ["6919126084284083155"]}, "iRHBUsgAAAAJ:RGFaLdJalmkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Accompanied singing voice synthesis with fully text-controlled melody", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:RGFaLdJalmkC", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11725362032593030706", "cites_id": ["11725362032593030706"]}, "iRHBUsgAAAAJ:EUQCXRtRnyEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Omniaudio: Generating spatial audio from 360-degree video", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:EUQCXRtRnyEC", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4088694552939636976", "cites_id": ["4088694552939636976"]}, "iRHBUsgAAAAJ:pqnbT2bcN3wC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Flashaudio: Rectified flows for fast and high-fidelity text-to-audio generation", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:pqnbT2bcN3wC", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16163085057520483867", "cites_id": ["16163085057520483867"]}, "iRHBUsgAAAAJ:R3hNpaxXUhUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Diffusion denoising process for perceptron bias in out-of-distribution detection", "pub_year": "2022"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:R3hNpaxXUhUC", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10567998091823456008", "cites_id": ["10567998091823456008"]}, "iRHBUsgAAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A rd-t network for hand gesture recognition based on millimeter-wave sensor", "pub_year": "2020"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:d1gkVwhDpl0C", "num_citations": 7, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8141374826143006366", "cites_id": ["8141374826143006366"]}, "iRHBUsgAAAAJ:SeFeTyx0c_EC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:SeFeTyx0c_EC", "num_citations": 6, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15654564753412480687", "cites_id": ["15654564753412480687"]}, "iRHBUsgAAAAJ:ZHo1McVdvXMC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Text-to-song: Towards controllable music generation incorporating vocals and accompaniment", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:ZHo1McVdvXMC", "num_citations": 6, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=16688761447778487861", "cites_id": ["16688761447778487861"]}, "iRHBUsgAAAAJ:k_IJM867U9cC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Molecule-Space: Free Lunch in Unified Multimodal Space via Knowledge Fusion", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:k_IJM867U9cC", "num_citations": 6, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15657380580640127305", "cites_id": ["15657380580640127305"]}, "iRHBUsgAAAAJ:cFHS6HbyZ2cC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Omnichat: Enhancing spoken dialogue systems with scalable synthetic data for diverse scenarios", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:cFHS6HbyZ2cC", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7494014608082809312", "cites_id": ["7494014608082809312"]}, "iRHBUsgAAAAJ:3s1wT3WcHBgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Momu-diffusion: On learning long-term motion-music synchronization and correspondence", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:3s1wT3WcHBgC", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14227336540975254571", "cites_id": ["14227336540975254571"]}, "iRHBUsgAAAAJ:Wp0gIr-vW9MC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Wav2sql: Direct generalizable speech-to-sql parsing", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:Wp0gIr-vW9MC", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8348701222387314867", "cites_id": ["8348701222387314867"]}, "iRHBUsgAAAAJ:abG-DnoFyZgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Versatile framework for song generation with prompt-based control", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:abG-DnoFyZgC", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5909079625653090521", "cites_id": ["5909079625653090521"]}, "iRHBUsgAAAAJ:35N4QoGY0k4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Integrating Audio, Visual, and Semantic Information for Enhanced Multimodal Speaker Diarization", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:35N4QoGY0k4C", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5291123362886389066", "cites_id": ["5291123362886389066"]}, "iRHBUsgAAAAJ:J_g5lzvAfSwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Medic: Zero-shot music editing with disentangled inversion control", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:J_g5lzvAfSwC", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10706180370784119878", "cites_id": ["10706180370784119878"]}, "iRHBUsgAAAAJ:qxL8FJ1GzNcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Alignsts: Speech-to-singing conversion via cross-modal alignment", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:qxL8FJ1GzNcC", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=3124350401729850271", "cites_id": ["3124350401729850271"]}, "iRHBUsgAAAAJ:YFjsv_pBGBYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Self-supervised singing voice pre-training towards speech-to-singing conversion", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:YFjsv_pBGBYC", "num_citations": 3, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10274893363351855377", "cites_id": ["10274893363351855377"]}, "iRHBUsgAAAAJ:dfsIfKJdRG4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "T2A-Feedback: Improving Basic Capabilities of Text-to-Audio Generation via Fine-grained AI Feedback", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:dfsIfKJdRG4C", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12315614978914049800", "cites_id": ["12315614978914049800"]}, "iRHBUsgAAAAJ:RYcK_YlVTxYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "VoiceTuner: Self-Supervised Pre-training and Efficient Fine-tuning For Voice Generation", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:RYcK_YlVTxYC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2734032292025936296", "cites_id": ["2734032292025936296"]}, "iRHBUsgAAAAJ:xtRiw3GOFMkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:xtRiw3GOFMkC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=10213162201729536523", "cites_id": ["10213162201729536523"]}, "iRHBUsgAAAAJ:D03iK_w7-QYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:D03iK_w7-QYC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=8033076143114662575", "cites_id": ["8033076143114662575"]}, "iRHBUsgAAAAJ:u_35RYKgDlwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "AVSET-10M: An Open Large-Scale Audio-Visual Dataset with High Correspondence", "pub_year": "2024"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:u_35RYKgDlwC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17746476324210936114", "cites_id": ["17746476324210936114"]}, "iRHBUsgAAAAJ:isC4tDSrTZIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "TETA: Temporal-Enhanced Text-to-Audio Generation", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:isC4tDSrTZIC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12856513641027503156", "cites_id": ["12856513641027503156"]}, "iRHBUsgAAAAJ:a0OBvERweLwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Prosody-TTS: Self-Supervised Prosody Pretraining with Latent Diffusion For Text-to-Speech", "pub_year": "2023"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:a0OBvERweLwC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=14822319308969153582", "cites_id": ["14822319308969153582"]}, "iRHBUsgAAAAJ:nb7KW1ujOQ8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Integrating Audio, Visual, and Semantic Information for Enhanced Multimodal Speaker Diarization on Multi-party Conversation", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:nb7KW1ujOQ8C", "num_citations": 0}, "iRHBUsgAAAAJ:b0M2c_1WBrUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Unleashing the Power of Natural Audio Featuring Multiple Sound Sources", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:b0M2c_1WBrUC", "num_citations": 0}, "iRHBUsgAAAAJ:_xSYboBqXhAC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "NAT3DSound: 3D Spatial Sound Field Synthesis with Multi-Modal Non-Autoregressive Transformer", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:_xSYboBqXhAC", "num_citations": 0}, "iRHBUsgAAAAJ:KxtntwgDAa4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Robust Speech-Driven Body Language Generation", "pub_year": "2025"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:KxtntwgDAa4C", "num_citations": 0}, "iRHBUsgAAAAJ:4OULZ7Gr8RgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MultiBand: Multi-Task Song Generation with Personalized Prompt-Based Control"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:4OULZ7Gr8RgC", "num_citations": 0}, "iRHBUsgAAAAJ:fPk4N6BV_jEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Noise-Robust Audio-Visual Speech-Driven Body Language Synthesis"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:fPk4N6BV_jEC", "num_citations": 0}, "iRHBUsgAAAAJ:TFP_iSt0sucC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "MVoice: Multilingual Unified Voice Generation With Discrete Representation at Scale"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:TFP_iSt0sucC", "num_citations": 0}, "iRHBUsgAAAAJ:iH-uZ7U-co4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "HarmonyLM: Advancing Unified Large-Scale Language Modeling for Sound and Music Generation"}, "filled": false, "author_pub_id": "iRHBUsgAAAAJ:iH-uZ7U-co4C", "num_citations": 0}}, "citedby5y": 3840, "hindex": 29, "hindex5y": 29, "i10index": 55, "i10index5y": 55, "cites_per_year": {"2022": 93, "2023": 542, "2024": 1419, "2025": 1775}, "updated": "2025-10-24 08:29:05.298814"}